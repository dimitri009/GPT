# ğŸ§  GPT from Scratch â€” Based on *"Build a Large Language Model (From Scratch)"*

This repository contains an implementation of a GPT-style language model from scratch, following the book **"Build a Large Language Model (From Scratch)"** by *Sebastian Raschka*. It includes training code, utilities, and a notebook to load GPT-2 weights from OpenAI.
![GPT](fig/GPT.jpg)

[image credits](https://www.researchgate.net/publication/373352176_FLUID-GPT_Fast_Learning_to_Understand_and_Investigate_Dynamics_with_a_Generative_Pre-Trained_Transformer_Efficient_Predictions_of_Particle_Trajectories_and_Erosion)

---

## ğŸ“– Project Overview

- **Goal**: Train a GPT-style transformer model from scratch on a real-world dataset.
- **Data**: [*The Verdict* by Alice Meynell](https://en.wikisource.org/wiki/The_Verdict) (public domain).
- **Framework**: PyTorch
- **Tokenizer**: GPT-2 BPE tokenizer via `tiktoken`
- **Model**: GPT-124M architecture defined from first principles
- **Training**: Simple training loop with loss tracking and validation
- **Extras**: Jupyter notebook to load and compare against GPT-2 weights from OpenAI

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ GPT/ # GPT model definition and config
â”‚ â”œâ”€â”€ GPT.py
â”‚ â””â”€â”€ GPT_CONFIG.py
â”œâ”€â”€ data/ # Training data
â”‚ â””â”€â”€ the-verdict.txt
â”œâ”€â”€ data_loader/ # Data preprocessing and batching
â”‚ â””â”€â”€ data_loader.py
â”œâ”€â”€ loss/ # Custom loss functions for GPT training
â”‚ â””â”€â”€ loss.py
â”œâ”€â”€ networks/ # Transformer block components (e.g., LayerNorm, attention)
â”‚ â””â”€â”€ networks.py
â”œâ”€â”€ tools/ # Training loop and related utilities
â”‚ â””â”€â”€ program.py
â”œâ”€â”€ utils/ # Utility functions
â”‚ â”œâ”€â”€ utils.py
â”‚ â””â”€â”€ plot.py
â”œâ”€â”€gpt2.ipynb # Jupyter notebook
â”œâ”€â”€ train.py # Main training script
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## ğŸš€ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/dimitri009/GPT.git
cd GPT
```

### 2. Install dependencies

I recommend using a virtual environment:
```
pip install -r requirements.txt
```
    This project uses torch, tiktoken, matplotlib, and numpy.

### 3. Prepare the data

The dataset is already included:

    data/the-verdict.txt

If you want to download a fresh copy:
```
wget https://en.wikisource.org/wiki/The_Verdict -O data/the-verdict.txt
```
### 4. Train the model
```
python train.py
```
Training will log loss over epochs, plot the curves at the end and save the plot in `fig/`. You can customize hyperparameters in `train.py` and `GPT/GPT_CONFIG.py`.
### 5. Load GPT-2 Weights (Optional)

To load pretrained OpenAI GPT-2 weights and compare against your model, use the notebook:
```
jupyter notebook gpt2.ipynb
```
## ğŸ§ª Features

    Manual implementation of GPT-2 style attention layers

    Positional embeddings

    Token embeddings using tiktoken

    Train-validation splitting and loss plotting

    Option to use drop_last, stride, and context length control in dataloader

    GPT-2 weight loading notebook for comparison

### ğŸ“Š Example Output

Training loss and validation loss are plotted after training. Example:
![plot](fig/loss.png)

### ğŸ“š References

Book: [Build a Large Language Model (From Scratch) by Sebastian Raschka](https://www.manning.com/books/build-a-large-language-model-from-scratch)

Tokenizer: [OpenAI tiktoken](https://github.com/openai/tiktoken)

Dataset: [The Verdict - Wikisource](https://en.wikisource.org/wiki/The_Verdict)

## ğŸ› ï¸ TODO

* Add script for classifier fine-tuning (e.g., sentiment classification, topic classification)
* Add script for instruction fine-tuning (e.g., Q&A or task completion using promptâ€“response format)


## ğŸ§‘â€ğŸ’» License

This project is released under the MIT License. See LICENSE for details.
## ğŸ¤ Acknowledgements

* Sebastian Raschka for his detailed book and educational resources
* OpenAI for GPT-2 and the `tiktoken` tokenizer 
* [Project Gutenberg / Wikisource](https://www.gutenberg.org/) for public domain texts